# sber-test-work

## Установка зависимостей

Выполнить в корневой директории проекта:
```bash
pip install -r requirements.txt
```

Дополнительные библиотеки хромиума для Playwright требуются исключительно для использования продвинутого парсинга html-страниц с динамическим контентом и не обязательны для установки. Установить можно так:
```bash
playwright install
```

## Запуск приложения

Файл является обязательным аргументом. Выполнить в корневой директории проекта:
```bash
python src/main.py <file>
```

## Библиотеки

- requests - сердце проекта. Большая часть запросов производилась через него. Удобный интерфейс и легок в использовании.
- playwright - нужный инструмент для парса html-страниц с динамическим контентом. Выбрал для этой задачи его потому что он до сих пор поддерживается в отличие от аналогов.
- pypdf - наследник PyPDF2 и PyPDF4. Выбрал опять же потому что до сих пор поддерживается.
- langdetect - ?
- beautifulsoup4 - удобная и быстрая очистка html-страниц от html-тегов
- python-docx - для обработки DocX документов
- openpyxl - для обработки XLSX документов

## Допущения и упрощения

- Программа разработана с тем учетом, что файлы будут не очень большие. Не разработана порционная (по чанкам) загрузка файлов и их дальнейшая обработка. Все файлы загружаются и обрабатываются целиком, как есть.
- Также к предыдущему пункту. Предполагается, что не будет такой ситуации, когда соединение разрывается или происходит что-то похожее. Не разработано продолжение загрузки, которое позволит загрузить файл до конца.
- Можно обработать только .pdf, .docx и .xlsx документы. Даже обработка "plain/text" отсутствует.
- Незначащими query-параметрами являются только "\*clid" (click id), "utm_\*" (UTM-метки), "cache_\*" (метки для кэширования) и "*_debug" (отладочные). Допускаю, что есть и множество других.

## Возникшие сложности

- В первую очередь самое сложное было сделать более менее нормальные интерфейсы и написать архитектуру, чтобы было удобно взаимодействовать со всем этим.
- Так как архитектура была слабоватой, я потратил достаточно много времени на то, что бы определить каким способом будет лучше доставить кучу данных из пункта А в пункт Б и при этом не возить везде с собой целый список из датаклассов.
- Много новых библиотек, а именно тех, с которыми я либо не работал, либо работал мало, либо даже не слышал о таких. Пришлось почитать много документаций, чтобы написать хотя бы что-то или найти нужную библиотеку из набора аналогов.

## Мысли по улучшению и развитию

Я старался сделать проект максимально модульным и слабосвязанным, и на данном этапе вижу, что мое решение может впоследствии как минимум дать возможность добавить новые способы обработки информации, новые способы скачивания и т.д.
Так как download'еры и handler'ы не связаны с основной целью проекта, мне кажется они могут быть неплохой базой для решения иных задач.
Также я хотел бы видеть в продолжении проекта веб-приложение с веб-интерфейсом с воможностью управления сбором данных и отображением результатов.
Если в проект добавить асинхронность/мультипоточность/мультипроцессность, то можно неплохо масштабировать, например распределить задачи сбора, обработки информации, распределить по разным машинам/контейнерам и т.п.
Можно еще добавить новые механизмы обхода защиты от ботов, например те же прокси-серверы или ротацию User-Agent, чтобы улучшить производительность.
